{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Preprocessing\n",
        "\n",
        "In order to clean and preprocess the data, I created a data cleaning and preprocessing function with the following capabilities:\n",
        "\n",
        "- Stopword removal\n",
        "- Lemmatization\n",
        "- Lowercase\n",
        "- Punctuation cleaning\n",
        "- Number cleaning"
      ],
      "metadata": {
        "id": "Jh-ZJyAwJaoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Individual cleaning functions\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  words=word_tokenize(text)\n",
        "  sentence=[w for w in words if w not in stop_words]\n",
        "  return \" \".join(sentence)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  wordlist=[]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  sentences=sent_tokenize(text)\n",
        "  for sentence in sentences:\n",
        "      words=word_tokenize(sentence)\n",
        "      for word in words:\n",
        "          wordlist.append(lemmatizer.lemmatize(word))\n",
        "  return ' '.join(wordlist)\n",
        "\n",
        "def lowercase_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  additional_punctuations = ['’', '…'] # punctuations not in string.punctuation\n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, '')\n",
        "\n",
        "  for punctuation in additional_punctuations:\n",
        "    text = text.replace(punctuation, '')\n",
        "\n",
        "  return text\n",
        "\n",
        "def remove_numbers(text):\n",
        "  if text is not None:\n",
        "    text = text.replace(r'^\\d+\\.\\s+','')\n",
        "\n",
        "  text = re.sub(\"[0-9]\", '', text)\n",
        "  return text\n",
        "\n",
        "# Unified boolean controlled cleaning function\n",
        "def clean_and_preprocess_data(text, lowercase=True, clean_stopwords=True, clean_punctuations=True, clean_links=True,\n",
        "                              clean_emojis=True, clean_spaces=True, clean_numbers=True,  lemmatize=True):\n",
        "\n",
        "  if clean_stopwords == True:\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "  if clean_punctuations == True:\n",
        "    text = remove_punctuations(text)\n",
        "\n",
        "  if clean_numbers == True:\n",
        "    text = remove_numbers(text)\n",
        "\n",
        "  if lemmatize == True:\n",
        "    text = lemmatize_text(text)\n",
        "\n",
        "  if lowercase == True:\n",
        "    return text.lower()\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "6fcQ4N41J3KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Preprocessing and cleaning the raw data"
      ],
      "metadata": {
        "id": "utW4rbDzJ-Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"subtitles.csv\")\n",
        "\n",
        "data['subtitles'] = data['subtitles'].apply(lambda x: clean_and_preprocess_data(x, lemmatize=False, clean_numbers=False, clean_stopwords=False, clean_punctuations=False, lowercase=False))\n",
        "print(text_df.head())\n",
        "\n",
        "# Saving the preprocessed data into a .csv file\n",
        "data.to_csv(\"subtitles_cleaned.csv\")"
      ],
      "metadata": {
        "id": "zja_qIJoKJRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}